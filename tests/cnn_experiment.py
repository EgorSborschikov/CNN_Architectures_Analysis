#!/usr/bin/env python3
"""
–ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ CNN - –ü–†–ê–í–ò–õ–¨–ù–´–ï –†–ê–ó–ú–ï–†–´
"""

import os
import sys
import time
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_lfw_people
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

# –û—Ç–∫–ª—é—á–∞–µ–º warning'–∏ torchvision
import warnings
warnings.filterwarnings('ignore', category=UserWarning)

print("="*80)
print("–≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢: CNN –î–õ–Ø –†–ê–°–ü–û–ó–ù–ê–í–ê–ù–ò–Ø –õ–ò–¶ (LFW –î–ê–¢–ê–°–ï–¢)")
print("="*80)

# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
os.makedirs('results/cnn', exist_ok=True)

# ============================================================================
# 1. –î–ê–ù–ù–´–ï
# ============================================================================
print("\n[1/6] –ó–ê–ì–†–£–ó–ö–ê –ò –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•")

start_time = time.time()
lfw_data = fetch_lfw_people(min_faces_per_person=70, resize=0.4)
X = lfw_data.images
y = lfw_data.target
target_names = lfw_data.target_names

print(f"   ‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(X)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
print(f"   ‚úì –ö–ª–∞—Å—Å–æ–≤: {len(target_names)}")
print(f"   ‚úì –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {X[0].shape}")
print(f"   ‚úì –°–ø–∏—Å–æ–∫ –ª—é–¥–µ–π: {', '.join(target_names)}")

# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –∫–∞–Ω–∞–ª–∞
X = X / 255.0  # [0, 1]
X = X[:, np.newaxis, :, :]  # (n_samples, 1, height, width)

print(f"   ‚úì –§–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö: {X.shape}")

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42
)

print(f"\n   –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:")
print(f"     –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞:  {len(X_train):4d} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
print(f"     –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_val):4d} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
print(f"     –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞:    {len(X_test):4d} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")

# –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ç–µ–Ω–∑–æ—Ä—ã
X_train_t = torch.FloatTensor(X_train)
y_train_t = torch.LongTensor(y_train)
X_val_t = torch.FloatTensor(X_val)
y_val_t = torch.LongTensor(y_val)
X_test_t = torch.FloatTensor(X_test)
y_test_t = torch.LongTensor(y_test)

data_load_time = time.time() - start_time
print(f"   ‚úì –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã –∑–∞ {data_load_time:.1f} —Å–µ–∫—É–Ω–¥")

# ============================================================================
# 2. –ú–û–î–ï–õ–¨ - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê
# ============================================================================
print("\n[2/6] –°–û–ó–î–ê–ù–ò–ï –ú–û–î–ï–õ–ò CNN (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞)")

class EfficientFaceCNN(nn.Module):
    """–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è CNN –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ª–∏—Ü - –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ï –†–ê–ó–ú–ï–†–´"""
    def __init__(self, num_classes, input_height=50, input_width=37, dropout_rate=0.3):
        super().__init__()
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–º–µ—Ä—ã –ø–æ—Å–ª–µ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–ª–æ–µ–≤
        # –ù–∞—á–∏–Ω–∞–µ–º —Å: 1 x 50 x 37
        
        # –ü–æ—Å–ª–µ conv1 + pool: 32 x 25 x 18 (50/2=25, 37/2=18.5 -> 18)
        # –ü–æ—Å–ª–µ conv2 + pool: 64 x 12 x 9 (25/2=12.5 -> 12, 18/2=9)
        # –ü–æ—Å–ª–µ conv3 + pool: 128 x 6 x 4 (12/2=6, 9/2=4.5 -> 4)
        
        # Feature extractor
        self.features = nn.Sequential(
            # Block 1: 1x50x37 -> 32x25x18
            nn.Conv2d(1, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout2d(0.1),
            
            # Block 2: 32x25x18 -> 64x12x9
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout2d(0.2),
            
            # Block 3: 64x12x9 -> 128x6x4
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
        )
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∏—á–µ–π –ø–æ—Å–ª–µ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–ª–æ–µ–≤
        # 128 –∫–∞–Ω–∞–ª–æ–≤ * 6 –≤—ã—Å–æ—Ç–∞ * 4 —à–∏—Ä–∏–Ω–∞ = 3072
        self.feature_size = 128 * 6 * 4
        
        # Classifier
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(self.feature_size, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate),
            nn.Linear(256, num_classes)
        )
        
        print(f"     –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–∞: 1x{input_height}x{input_width}")
        print(f"     –†–∞–∑–º–µ—Ä —Ñ–∏—á–µ–π –ø–æ—Å–ª–µ conv —Å–ª–æ–µ–≤: {self.feature_size}")
    
    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x

# –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = EfficientFaceCNN(len(target_names)).to(device)

# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"   ‚úì –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: EfficientFaceCNN")
print(f"   ‚úì –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
print(f"   ‚úì –í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
print(f"   ‚úì –û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {trainable_params:,}")

# ============================================================================
# 3. –û–ë–£–ß–ï–ù–ò–ï
# ============================================================================
print("\n[3/6] –ù–ê–°–¢–†–û–ô–ö–ê –û–ë–£–ß–ï–ù–ò–Ø")

criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)

batch_size = 16  # –£–º–µ–Ω—å—à–∏–ª–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
epochs = 10      # –£–º–µ–Ω—å—à–∏–ª–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞

print(f"   ‚úì Loss —Ñ—É–Ω–∫—Ü–∏—è: CrossEntropyLoss")
print(f"   ‚úì –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: AdamW (lr=0.001)")
print(f"   ‚úì Scheduler: CosineAnnealingLR")
print(f"   ‚úì Batch size: {batch_size}")
print(f"   ‚úì –≠–ø–æ—Ö: {epochs}")

def get_batches(X, y, batch_size, shuffle=True):
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –±–∞—Ç—á–µ–π"""
    n_samples = len(X)
    indices = torch.randperm(n_samples) if shuffle else torch.arange(n_samples)
    
    for i in range(0, n_samples, batch_size):
        batch_idx = indices[i:i+batch_size]
        yield X[batch_idx], y[batch_idx]

# ============================================================================
# 4. –¶–ò–ö–õ –û–ë–£–ß–ï–ù–ò–Ø
# ============================================================================
print("\n[4/6] –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø")

history = {
    'train_loss': [], 'train_acc': [],
    'val_loss': [], 'val_acc': [],
    'lr': []
}

best_val_acc = 0.0
best_model_state = None

for epoch in range(epochs):
    epoch_start = time.time()
    
    # --- –û–ë–£–ß–ï–ù–ò–ï ---
    model.train()
    train_loss = 0.0
    train_correct = 0
    train_total = 0
    num_batches = 0
    
    for X_batch, y_batch in get_batches(X_train_t, y_train_t, batch_size, shuffle=True):
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä—ã
        if num_batches == 0:
            print(f"     –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {X_batch.shape}")
        
        optimizer.zero_grad()
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()
        
        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_total += y_batch.size(0)
        train_correct += (predicted == y_batch).sum().item()
        num_batches += 1
    
    train_loss = train_loss / num_batches if num_batches > 0 else 0
    train_acc = 100. * train_correct / train_total if train_total > 0 else 0
    
    # --- –í–ê–õ–ò–î–ê–¶–ò–Ø ---
    model.eval()
    val_loss = 0.0
    val_correct = 0
    val_total = 0
    num_val_batches = 0
    
    with torch.no_grad():
        for X_batch, y_batch in get_batches(X_val_t, y_val_t, batch_size, shuffle=False):
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            
            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_total += y_batch.size(0)
            val_correct += (predicted == y_batch).sum().item()
            num_val_batches += 1
    
    val_loss = val_loss / num_val_batches if num_val_batches > 0 else 0
    val_acc = 100. * val_correct / val_total if val_total > 0 else 0
    
    # --- –°–û–•–†–ê–ù–ï–ù–ò–ï –ò–°–¢–û–†–ò–ò ---
    history['train_loss'].append(train_loss)
    history['train_acc'].append(train_acc)
    history['val_loss'].append(val_loss)
    history['val_acc'].append(val_acc)
    history['lr'].append(optimizer.param_groups[0]['lr'])
    
    # --- –°–û–•–†–ê–ù–ï–ù–ò–ï –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò ---
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        best_model_state = model.state_dict().copy()
    
    # --- –í–´–í–û–î –ò–ù–§–û–†–ú–ê–¶–ò–ò ---
    epoch_time = time.time() - epoch_start
    
    print(f"\n   –≠–ø–æ—Ö–∞ {epoch+1:2d}/{epochs}:")
    print(f"     Train: Loss={train_loss:.4f}, Acc={train_acc:.2f}%")
    print(f"     Val:   Loss={val_loss:.4f}, Acc={val_acc:.2f}%")
    print(f"     LR:    {optimizer.param_groups[0]['lr']:.6f}")
    print(f"     –í—Ä–µ–º—è: {epoch_time:.1f} —Å–µ–∫")
    
    if val_acc == best_val_acc:
        print(f"     ‚úÖ –ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å!")
    
    # --- –û–ë–ù–û–í–õ–ï–ù–ò–ï SCHEDULER ---
    scheduler.step()

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
if best_model_state:
    model.load_state_dict(best_model_state)

print(f"\n   ‚úì –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
print(f"   ‚úì –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {best_val_acc:.2f}%")

# ============================================================================
# 5. –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï
# ============================================================================
print("\n[5/6] –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–ê –¢–ï–°–¢–û–í–û–ô –í–´–ë–û–†–ö–ï")

model.eval()
test_loss = 0.0
test_correct = 0
test_total = 0
all_predictions = []
all_true_labels = []
num_test_batches = 0

with torch.no_grad():
    for X_batch, y_batch in get_batches(X_test_t, y_test_t, batch_size, shuffle=False):
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        
        test_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        test_total += y_batch.size(0)
        test_correct += (predicted == y_batch).sum().item()
        
        all_predictions.extend(predicted.cpu().numpy())
        all_true_labels.extend(y_batch.cpu().numpy())
        num_test_batches += 1

test_loss = test_loss / num_test_batches if num_test_batches > 0 else 0
test_acc = 100. * test_correct / test_total if test_total > 0 else 0

print(f"\n   –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø:")
print(f"   {'='*40}")
print(f"     Test Loss:     {test_loss:.4f}")
print(f"     Test Accuracy: {test_acc:.2f}%")
print(f"     Correct/Total: {test_correct}/{test_total}")

# –û—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
print(f"\n   –û–¢–ß–ï–¢ –ü–û –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò:")
print(f"   {'='*40}")
if len(all_true_labels) > 0:
    print(classification_report(all_true_labels, all_predictions, 
                              target_names=target_names, digits=3))
else:
    print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç—á–µ—Ç–∞")

# ============================================================================
# 6. –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –ò –°–û–•–†–ê–ù–ï–ù–ò–ï
# ============================================================================
print("\n[6/6] –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –ò –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")

# 6.1. –ì—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Loss
axes[0, 0].plot(history['train_loss'], 'b-', linewidth=2, label='Train Loss')
axes[0, 0].plot(history['val_loss'], 'r-', linewidth=2, label='Val Loss')
axes[0, 0].set_title('–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å', fontsize=14, fontweight='bold')
axes[0, 0].set_xlabel('–≠–ø–æ—Ö–∞')
axes[0, 0].set_ylabel('Loss')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# Accuracy
axes[0, 1].plot(history['train_acc'], 'b-', linewidth=2, label='Train Accuracy')
axes[0, 1].plot(history['val_acc'], 'r-', linewidth=2, label='Val Accuracy')
axes[0, 1].axhline(y=best_val_acc, color='g', linestyle='--', alpha=0.7, 
                   label=f'Best Val: {best_val_acc:.1f}%')
axes[0, 1].set_title('–¢–æ—á–Ω–æ—Å—Ç—å', fontsize=14, fontweight='bold')
axes[0, 1].set_xlabel('–≠–ø–æ—Ö–∞')
axes[0, 1].set_ylabel('Accuracy (%)')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# Learning Rate
axes[1, 0].plot(history['lr'], 'g-', linewidth=2, marker='o', markersize=4)
axes[1, 0].set_title('–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è', fontsize=14, fontweight='bold')
axes[1, 0].set_xlabel('–≠–ø–æ—Ö–∞')
axes[1, 0].set_ylabel('Learning Rate')
axes[1, 0].grid(True, alpha=0.3)

# –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ (–µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ)
if len(all_true_labels) > 0:
    cm = confusion_matrix(all_true_labels, all_predictions)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 1],
                xticklabels=target_names, yticklabels=target_names)
    axes[1, 1].set_title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫', fontsize=14, fontweight='bold')
    axes[1, 1].set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏')
    axes[1, 1].set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏')
else:
    axes[1, 1].text(0.5, 0.5, '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö\n–¥–ª—è –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫', 
                    ha='center', va='center', fontsize=12)
    axes[1, 1].set_title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫', fontsize=14, fontweight='bold')

plt.suptitle(f'CNN –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ª–∏—Ü (LFW –¥–∞—Ç–∞—Å–µ—Ç)\n'
             f'Test Accuracy: {test_acc:.2f}% | Model: EfficientFaceCNN', 
             fontsize=16, fontweight='bold')
plt.tight_layout()
plt.savefig('results/cnn/training_results.png', dpi=150, bbox_inches='tight')

# 6.2. –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
fig, axes = plt.subplots(2, 4, figsize=(15, 8))

model.eval()
with torch.no_grad():
    displayed = 0
    for i in range(min(8, len(X_test))):
        image = X_test[i].squeeze()
        true_label = y_test[i]
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        input_tensor = torch.FloatTensor(X_test[i:i+1]).to(device)
        output = model(input_tensor)
        probs = torch.softmax(output, 1)
        pred_prob, pred_label = torch.max(probs, 1)
        
        ax = axes[i // 4, i % 4]
        ax.imshow(image, cmap='gray')
        
        true_name = target_names[true_label]
        pred_name = target_names[pred_label.item()]
        
        color = 'green' if true_label == pred_label.item() else 'red'
        border_color = color
        
        ax.set_title(f"True: {true_name}\nPred: {pred_name}\nProb: {pred_prob.item():.2f}", 
                     fontsize=9, color=color)
        ax.axis('off')
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–º–∫—É
        for spine in ax.spines.values():
            spine.set_edgecolor(border_color)
            spine.set_linewidth(3)
        
        displayed += 1
    
    # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –ø—É—Å—Ç—ã–µ –º–µ—Å—Ç–∞
    for i in range(displayed, 8):
        ax = axes[i // 4, i % 4]
        ax.axis('off')

plt.suptitle('–ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig('results/cnn/prediction_examples.png', dpi=150, bbox_inches='tight')

# 6.3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
torch.save({
    'model_state_dict': model.state_dict(),
    'target_names': target_names,
    'test_accuracy': test_acc,
    'history': history,
    'model_config': {
        'num_classes': len(target_names),
        'total_params': total_params,
        'input_shape': X.shape[1:]
    }
}, 'results/cnn/face_recognition_model.pth')

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
import json
results_summary = {
    'experiment': 'CNN Face Recognition',
    'dataset': 'LFW',
    'date': time.strftime('%Y-%m-%d %H:%M:%S'),
    'device': str(device),
    'model': 'EfficientFaceCNN',
    'parameters': total_params,
    'best_val_accuracy': best_val_acc,
    'test_accuracy': test_acc,
    'test_loss': test_loss,
    'training_epochs': epochs,
    'batch_size': batch_size,
    'num_classes': len(target_names),
    'class_names': list(target_names),
    'sample_counts': {
        'train': len(X_train),
        'val': len(X_val),
        'test': len(X_test)
    }
}

with open('results/cnn/experiment_summary.json', 'w') as f:
    json.dump(results_summary, f, indent=2)

print(f"\n   ‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
print(f"     - –ì—Ä–∞—Ñ–∏–∫–∏: results/cnn/training_results.png")
print(f"     - –ü—Ä–∏–º–µ—Ä—ã: results/cnn/prediction_examples.png")
print(f"     - –ú–æ–¥–µ–ª—å:  results/cnn/face_recognition_model.pth")
print(f"     - –ú–µ—Ç—Ä–∏–∫–∏: results/cnn/experiment_summary.json")

# ============================================================================
# –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢
# ============================================================================
print("\n" + "="*80)
print("–≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!")
print("="*80)
print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
print(f"   {'‚îÄ'*40}")
print(f"   ‚îÇ –î–∞—Ç–∞—Å–µ—Ç:          LFW ({len(X)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)")
print(f"   ‚îÇ –ö–ª–∞—Å—Å—ã:           {len(target_names)} —á–µ–ª–æ–≤–µ–∫")
print(f"   ‚îÇ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:      EfficientFaceCNN")
print(f"   ‚îÇ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:        {total_params:,}")
print(f"   ‚îÇ –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ:       {device}")
print(f"   {'‚îÄ'*40}")
print(f"   ‚îÇ –õ—É—á—à–∞—è Val Acc:   {best_val_acc:.2f}%")
print(f"   ‚îÇ Test Accuracy:    {test_acc:.2f}%")
print(f"   ‚îÇ Test Loss:        {test_loss:.4f}")
print(f"   {'‚îÄ'*40}")

print(f"\n‚úÖ –ü–ï–†–í–´–ô –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ (CNN) –ó–ê–í–ï–†–®–ï–ù!")
print("="*80)

plt.show()